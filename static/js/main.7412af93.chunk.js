(this.webpackJsonpmyportfolio=this.webpackJsonpmyportfolio||[]).push([[0],{104:function(e,t,a){e.exports=a.p+"static/media/hero.9f089f11.png"},113:function(e,t,a){e.exports=a(144)},118:function(e,t,a){},144:function(e,t,a){"use strict";a.r(t);var n=a(0),l=a.n(n),r=a(9),i=a.n(r),o=(a(118),a(105)),s=a(187),c=a(68),m=a(10),d=a(62),u=a(5),p=(a(36),a(166)),h=a(167),g=a(168),f=a(169),y=a(96),E=a.n(y),w=function(e){var t=e.title,a=void 0===t?"":t,n=e.backUrl,r=void 0===n?"":n,i=Object(m.f)(),o=Object(p.a)("(max-width:600px)");return l.a.createElement(h.a,{position:"fixed",style:{backgroundColor:"#ffffff"}},l.a.createElement(g.a,null,l.a.createElement(f.a,{container:!0},l.a.createElement(f.a,{item:!0,xs:4,md:1,style:{display:"flex",alignItems:"center"}},l.a.createElement("div",{style:{display:"flex",alignItems:"center",cursor:"pointer"},onClick:function(){i.push(r)}},l.a.createElement(E.a,{color:"secondary"}),l.a.createElement(d.a,{variant:"h5",color:"secondary"},"Back"))),l.a.createElement(f.a,{item:!0,xs:8,md:10,style:{display:"flex",justifyContent:"center"}},l.a.createElement(d.a,{variant:o?"h6":"h4",color:"primary"},a)),l.a.createElement(f.a,{item:!0,xs:0,md:1}))))},b=[{path:"/article/snn",component:function(e){return l.a.createElement("div",null,l.a.createElement(w,{title:"Shallow Neural Network",backUrl:"/"}),l.a.createElement("div",{className:"article-content"},l.a.createElement(d.a,null,"Shallow Neural Network"),l.a.createElement(d.a,null,"Logistic Regression for Binary Classification"),l.a.createElement(d.a,null,"let us say there are m examples for training. (x, y ) where x belongs to R, y belongs to ",1),l.a.createElement(d.a,null,"Logistic Regression is given x, y^ = P(y = 1 | x)"),l.a.createElement(d.a,null,"Parameters: w belongs to R, b belongs to R Output: y^ = wT*x + b (for Regression)"),l.a.createElement(d.a,null,"Output: y^ = sigmoid(wT*x + b) (for Classification)"),l.a.createElement(d.a,null,"Sigmoid(z) = 1 / 1 + e**(-z)"),l.a.createElement(d.a,null,"Loss (error) function"),l.a.createElement(d.a,null,"For simple functions: L(y^, y) = 1 / z * (y^ - y)**2"),l.a.createElement(d.a,null,"For logistic regression it becomes non convex, therefore it makes it hard for gradient descent as there are multiple local maxima"),l.a.createElement(d.a,null,"L(y^, y) = -(y log(y^) + (1-y)log(1-y^)) (This will make it convex)"),l.a.createElement(d.a,null,"Cost Function"),l.a.createElement(d.a,null,"J(w,b) = 1/m Sum(i = 1 to m) (L(y^(i), y(i)))"),l.a.createElement(d.a,null,"J(w,b) = -1/m Sum(i = 1 to m) (-(y log(y^) + (1-y)log(1-y^)))"),l.a.createElement(d.a,null,"Gradient Descent"),l.a.createElement(d.a,null,"w = w - alpha * (dJ(w,b)/dw)"),l.a.createElement(d.a,null,"b = b - alpha * (dJ(w,b)/db)"),l.a.createElement(d.a,null,"Logistic Regression Gradient Descent"),l.a.createElement(d.a,null,"z = wT*x +b"),l.a.createElement(d.a,null,"y^ = a = sigmoid(z)"),l.a.createElement(d.a,null,"L(a,y) = -(y log(y^) + (1-y)log(1-y^))"),l.a.createElement(d.a,null,"L(a,y) = -(y log(y^) + (1-y)log(1-y^))"),l.a.createElement(d.a,null,"da = -y/(a) + (1-y)/(1-a)"),l.a.createElement(d.a,null,"dz = a - y"),l.a.createElement(d.a,null,"Logistic Regression for m examples (Pseudo Code)"),l.a.createElement(d.a,null,"J = 0, dw1 = 0, dw2 = 0, db = 0"),l.a.createElement(d.a,null,"for i = 1 to m:"),l.a.createElement(d.a,null," z(i) = wTx(i) + b"),l.a.createElement(d.a,null," a(i) = sigmoid(z(i))"),l.a.createElement(d.a,null," ","J += -[y(i)log(a(i)) + ((1 - y(i))(log(1 - a(i))))]"," "),l.a.createElement(d.a,null," for n features: "),l.a.createElement(d.a,null," dz(i) = a(i) - y(i) "),l.a.createElement(d.a,null," dw1 += x1(i) * dz(i) "),l.a.createElement(d.a,null," dw2 += x2(i) * dz(i) "),l.a.createElement(d.a,null," db += dz(i) "),l.a.createElement(d.a,null,"J /= m, dw1 /= m, dw2 /= m, db /= m"),l.a.createElement(d.a,null,"w1 = w1 - alpha(dw1)"),l.a.createElement(d.a,null,"w2 = w2 - alpha(dw2)"),l.a.createElement(d.a,null,"b = b - alpha(db)"),l.a.createElement(d.a,null,"Complexity = O(m * n)"),l.a.createElement(d.a,null,"We can make this O(1) by vectorizing"),l.a.createElement(d.a,null,"Vectorizing : "),l.a.createElement(d.a,null,"[z(1), z(2), ... , z(m)] = [w1, w2,..., wn](1 * n)[X1, X2, ... Xm](n * m) + b"),l.a.createElement(d.a,null,"A = [a1, a2, ..., am]"),l.a.createElement(d.a,null,"Y = [y1, y2, ..., ym]"),l.a.createElement(d.a,null,"dz = A - Y"),l.a.createElement(d.a,null,"db = 1 / m (np.sum(dz))"),l.a.createElement(d.a,null,"dw = 1 / m (X * dzT)"),l.a.createElement(d.a,null,"w = w - alpha(dw)"),l.a.createElement(d.a,null,"b = b - alpha(db)"),l.a.createElement(d.a,{variant:"h5"},"Shallow Neural Networks"),l.a.createElement(d.a,null,"In linear regression z & a are calculated only once."),l.a.createElement(d.a,null,"Whereas in a neural network they are calculated multiple times(i.e they are calculated on each layer) & finally we calculate loss at end."),l.a.createElement(d.a,null,"Activation functions:"),l.a.createElement(d.a,null,"Sigmoid, Tanh, ReLu, Leaky ReLu (Having Non linearity in activation functions helps it solve complex problems)"),l.a.createElement(d.a,null,"Gradient Descent & Back Prop"),l.a.createElement(d.a,null,"Z[1] = W[1]X + b[1] (X = A[0])"),l.a.createElement(d.a,null,"A[1] = g[1](Z[1])"),l.a.createElement(d.a,null,"Z[2] = W[2]X + b[2]"),l.a.createElement(d.a,null,"A[2] = g[2](Z[2])"),l.a.createElement(d.a,null,"Backprop:"),l.a.createElement(d.a,null,"dZ[2] = A[2] - Y"),l.a.createElement(d.a,null,"dW[2] = 1/m (dZ[2] * A[1]T)"),l.a.createElement(d.a,null,"db[2] = 1/m np.sum(dZ[2])"),l.a.createElement(d.a,null,"dZ[1] = W[2]T*dZ[2] * d(g[1])*(Z[1])"),l.a.createElement(d.a,null,"dW[1] = 1 / m * (dz[1]*XT)"),l.a.createElement(d.a,null,"db[1] = 1 / m * np.sum(dz[1])"),l.a.createElement(d.a,null,"Initializing w & b to 0 doesn't work as it'll make it symmetric"),l.a.createElement(d.a,null,"Parameters V/S Hyper parameters"),l.a.createElement(d.a,null,"Parameters : w[1], b[1], w[2], b[2] - Model will learn"),l.a.createElement(d.a,null,"Hyper Parameters : Learning Rate(alpha), # iterations, # hidden layers, # hidden units per layer, Choice of activation functions"),l.a.createElement(d.a,null,"Hyper parameters determine the final values of w & b"),l.a.createElement(d.a,null,"Other Hyper parameters : Momentum, Minibatch Size, Regularizations"),l.a.createElement(u.a,{text:'\n          model = tf.keras.Sequential([\n            tf.keras.layers.Dense(1)\n          ])\n\n          model.compile(loss=tf.keras.losses.mae,\n                        optimizer=tf.keras.optimizers.SGD(),\n                        metrics=["mae"])\n\n          model.fit(tf.expand_dims(X, axis=-1), y, epochs=5)\n\n          model.predict([17.0])\n          ',language:"python",showLineNumbers:!1,theme:u.b})))}},{path:"/article/dnn",component:function(e){return l.a.createElement("div",null,l.a.createElement(w,{title:"Deep Neural Network",backUrl:"/"}),l.a.createElement("div",{className:"article-content"},l.a.createElement(d.a,null,"Deep Neural Network Vectorized & Gradient Descent"),l.a.createElement(d.a,{className:"mt-16"},"Z[l] = W[l] * A[l-1] + b[l]"),l.a.createElement(d.a,{className:"mt-16"},"A[l] = g[l](Z[l])"),l.a.createElement(d.a,{className:"mt-16"},"dZ[l] = A[l] - Y or ( W[l]T*dZ[l] * d(g[l])*(Z[l]))"),l.a.createElement(d.a,null,"dW[l] = 1/m (dZ[l] * A[l - 1]T)"),l.a.createElement(d.a,null,"db[l] = 1/m np.sum(dZ[l])"),l.a.createElement(d.a,null,"w = w - alpha * (dw), b = b - alpha * (db)"),l.a.createElement(d.a,{className:"mt-16"},"Regression example"),l.a.createElement(u.a,{text:"\n          insurance_model_2 = tf.keras.Sequential([\n            tf.keras.layers.Dense(100),\n            tf.keras.layers.Dense(10),\n            tf.keras.layers.Dense(1)\n          ])\n\n          insurance_model_2.compile(loss=tf.keras.losses.mae,\n                                    optimizer=tf.keras.optimizers.Adam(),\n                                    metrics=['mae'])\n\n          history = insurance_model_2.fit(X_train, y_train, epochs=100, verbose=0)\n\n          insurance_model_2.evaluate(X_test, y_test)\n\n          insurance_model_2.summary()\n          ",language:"python",showLineNumbers:!1,theme:u.b}),l.a.createElement(u.a,{text:"\n          # summarize history for accuracy\n          plt.plot(history.history['accuracy'])\n          plt.plot(history.history['val_accuracy'])\n          plt.title('model accuracy')\n          plt.ylabel('accuracy')\n          plt.xlabel('epoch')\n          plt.legend(['train', 'test'], loc='upper left')\n          plt.show()\n          ",language:"python",showLineNumbers:!1,theme:u.b}),l.a.createElement(u.a,{text:"\n          # summarize history for loss\n          plt.plot(history.history['loss'])\n          plt.plot(history.history['val_loss'])\n          plt.title('model loss')\n          plt.ylabel('loss')\n          plt.xlabel('epoch')\n          plt.legend(['train', 'test'], loc='upper left')\n          plt.show()\n          ",language:"python",showLineNumbers:!1,theme:u.b}),l.a.createElement(d.a,{className:"mt-16"},"Classification example"),l.a.createElement(u.a,{text:"\n          model_1 = tf.keras.Sequential([\n            tf.keras.layers.Dense(1)\n          ])\n\n          model_1.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n                          optimizer=tf.keras.optimizers.SGD(),\n                          metrics=['accuracy'])\n\n          model_1.fit(X, y, epochs=5)\n          model_1.evaluate(X, y)\n          ",language:"python",showLineNumbers:!1,theme:u.b})))}},{path:"/article/cnn",component:function(e){return l.a.createElement("div",null,l.a.createElement(w,{title:"Convoluted Neural Network",backUrl:"/"}),l.a.createElement("div",{className:"article-content"},l.a.createElement(d.a,null,"Convoluted Neural Network"),l.a.createElement(d.a,{className:"mt-16"},"convolution on n x n image with f x f filters"),l.a.createElement(d.a,null,"padding p with stride s"),l.a.createElement(d.a,null,"floor((((n + 2p - f) / s) + 1), (((n + 2p - f) / s) + 1) )"),l.a.createElement(d.a,null," Same convolution v/s Valid convolution"),l.a.createElement(u.a,{text:'\n          train_datagen = ImageDataGenerator(rescale=1./255)\n          valid_datagen = ImageDataGenerator(rescale=1./255)\n\n          train_dir = "pizza_steak/train/"\n          test_dir = "pizza_steak/test/"\n\n          train_data = train_datagen.flow_from_directory(train_dir,\n                                                         batch_size=32,\n                                                         target_size=(224, 224),\n                                                         class_mode="binary",\n                                                         seed=42)\n\n          valid_data = valid_datagen.flow_from_directory(test_dir,\n                                                         batch_size=32,\n                                                         target_size=(224, 224),\n                                                         class_mode="binary",\n                                                         seed=42)\n\n          model_1 = tf.keras.models.Sequential([\n            tf.keras.layers.Conv2D(filters=10,\n                                   kernel_size=3,\n                                   activation="relu",\n                                   input_shape=(224, 224, 3)),\n            tf.keras.layers.Conv2D(10, 3, activation="relu"),\n            tf.keras.layers.MaxPool2D(pool_size=2,\n                                      padding="valid"),\n            tf.keras.layers.Conv2D(10, 3, activation="relu"),\n            tf.keras.layers.Conv2D(10, 3, activation="relu"),\n            tf.keras.layers.MaxPool2D(2),\n            tf.keras.layers.Flatten(),\n            tf.keras.layers.Dense(1, activation="sigmoid")\n          ])\n\n          model_1.compile(loss="binary_crossentropy",\n                        optimizer=tf.keras.optimizers.Adam(),\n                        metrics=["accuracy"])\n\n          history_1 = model_1.fit(train_data,\n                                  epochs=5,\n                                  steps_per_epoch=len(train_data),\n                                  validation_data=valid_data,\n                                  validation_steps=len(valid_data))\n          ',language:"python",showLineNumbers:!1,theme:u.b}),l.a.createElement(d.a,null,"Multi Class Classification"),l.a.createElement(u.a,{text:"\n          model_9 = Sequential([\n            Conv2D(10, 3, activation='relu', input_shape=(224, 224, 3)),\n            Conv2D(10, 3, activation='relu'),\n            MaxPool2D(),\n            Conv2D(10, 3, activation='relu'),\n            Conv2D(10, 3, activation='relu'),\n            MaxPool2D(),\n            Flatten(),\n            Dense(10, activation='softmax')\n          ])\n\n          # Compile the model\n          model_9.compile(loss=\"categorical_crossentropy\",\n                          optimizer=tf.keras.optimizers.Adam(),\n                          metrics=[\"accuracy\"])\n          history_9 = model_9.fit(train_data,\n            epochs=5,\n            steps_per_epoch=len(train_data),\n            validation_data=test_data,\n            validation_steps=len(test_data))\n          ",language:"python",showLineNumbers:!1,theme:u.b})))}},{path:"/article/rnn-1",component:function(e){return l.a.createElement("div",null,l.a.createElement(w,{title:"Recurrent Neural Network - NLP",backUrl:"/"}),l.a.createElement("div",{className:"article-content"},l.a.createElement(d.a,null,"Recurrent Neural Network - NLP"),l.a.createElement(d.a,{className:"mt-16"},"Why not a Standard Network?"),l.a.createElement(d.a,{className:"mt-16"},"- Inputs & Outputs can be of different lengths in different examples - Doesn't share features learned across different positions of text."),l.a.createElement(u.a,{text:"\n          a<t> = y^<t> = g(wa[a<t-1>, x<t>] + ba)\n          ",language:"python",showLineNumbers:!1,theme:u.b}),l.a.createElement(d.a,null,"Back propagation through time:"),l.a.createElement(u.a,{text:"\n          L<t>(y^<t>, y<t>) = -y<t> log(y^<t>) - (1 - y<t>)log(1 - y^<t>)\n          L = Sum (t = 1 to Ty) L<t>(y^<t>, y<t>)\n          ",language:"python",showLineNumbers:!1,theme:u.b}),l.a.createElement(d.a,null,"Types of RNNs:"),l.a.createElement(d.a,null," - Many to Many (Tx = Ty)"),l.a.createElement(d.a,null," - Many to One"),l.a.createElement(d.a,null," - One to Many"),l.a.createElement(d.a,null," - Many to Many (Tx != Ty)"),l.a.createElement(d.a,null,"Vanishing & Exploding Gradients"),l.a.createElement(d.a,null," ","- It is difficult for RNN to remember over a large portion of text"),l.a.createElement(d.a,null," ","- Hard for error to back propagate over a large text"),l.a.createElement(d.a,null," - Not able to capture long range dependencies"),l.a.createElement(d.a,null,"GRU"),l.a.createElement(u.a,{text:"\n          c<t> = a<t>\n          c~<t> = tanh(Wc[Phi * c<t - 1>, X<t>] + bc)\n          Phi(u) = sigmoid(Wu [c<t-1>, X<t>] + bu) #update\n          Phi(r) = sigmoid(Wr [c<t-1>, X<t>] + br) # relevance\n          c<t> = Phi(u) * c~<t> + (1 - Phi(u)) * c<t - 1>\n          ",language:"python",showLineNumbers:!1,theme:u.b}),l.a.createElement(d.a,null,"LSTM"),l.a.createElement(u.a,{text:"\n          c~<t> = tanh(Wc[a<t-1>, X<t>] + bc)\n          Phi(u) = sigmoid(Wn[a<t-1>, x<t>] + bc) #update\n          Phi(f) = sigmoid(Wf[a<t-1>, x<t>] + bf) #forget\n          Phi(o) = sigmoid(Wo[a<t-1>, x<t>] + bo) #output\n          c<t> = Phi(u) * c~<t> + Phi(f) * c<t-1>\n          a<t> = Phi(o) * c<t> or Phi(o) * tanh(c<t>)\n          ",language:"python",showLineNumbers:!1,theme:u.b}),l.a.createElement(d.a,null,"Bi-directional RNN"),l.a.createElement(u.a,{text:"\n            y^<t> = g(Wy[a(forward)<t>, a(backward)<t>] + by)\n          ",language:"python",showLineNumbers:!1,theme:u.b}),l.a.createElement(d.a,null,"Deep RNN"),l.a.createElement(d.a,null,"Multiple layers of any one of the architecture and each y<t> connected to a fully connected layer at the end"),l.a.createElement(d.a,null,"Feature Representation: Word Embedding"),l.a.createElement(d.a,null,"t-SNE, Word2Vec, Negative Sampling, Glove"),l.a.createElement(d.a,null,"Text Vectorization"),l.a.createElement(u.a,{text:'\n          import tensorflow as tf\n          from tensorflow.keras.layers import TextVectorization\n\n          text_vectorizer = TextVectorization(max_tokens=None,\n                                    standardize="lower_and_strip_punctuation",\n                                    split="whitespace",\n                                    ngrams=None,\n                                    output_mode="int",\n                                    output_sequence_length=None)\n                                    # pad_to_max_tokens=True)\n          ',language:"python",showLineNumbers:!1,theme:u.b}),l.a.createElement(u.a,{text:'\n          from tensorflow.keras import layers\n          model_2_embedding = layers.Embedding(input_dim=max_vocab_length,\n                                               output_dim=128,\n                                               embeddings_initializer="uniform",\n                                               input_length=max_length,\n                                               name="embedding_2")\n\n          inputs = layers.Input(shape=(1,), dtype="string")\n          x = text_vectorizer(inputs)\n          x = model_2_embedding(x)\n          # x = layers.LSTM(64, return_sequences=True)(x) # return vector for each word in the Tweet (you can stack RNN cells as long as return_sequences=True)\n          x = layers.LSTM(64)(x)\n          # x = layers.Dense(64, activation="relu")(x) # optional dense layer on top of output of LSTM cell\n          outputs = layers.Dense(1, activation="sigmoid")(x)\n          model_2 = tf.keras.Model(inputs, outputs, name="model_2_LSTM")\n\n\n          model_2.compile(loss="binary_crossentropy",\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=["accuracy"])\n\n          model_2.summary()\n\n          model_2_history = model_2.fit(train_sentences,\n            train_labels,\n            epochs=5,\n            validation_data=(val_sentences, val_labels),\n            callbacks=[create_tensorboard_callback(SAVE_DIR,\n                                                   "LSTM")])\n          ',language:"python",showLineNumbers:!1,theme:u.b}),l.a.createElement(d.a,null,"GRU"),l.a.createElement(u.a,{text:'\n          from tensorflow.keras import layers\n          model_3_embedding = layers.Embedding(input_dim=max_vocab_length,\n                                               output_dim=128,\n                                               embeddings_initializer="uniform",\n                                               input_length=max_length,\n                                               name="embedding_3")\n\n          # Build an RNN using the GRU cell\n          inputs = layers.Input(shape=(1,), dtype="string")\n          x = text_vectorizer(inputs)\n          x = model_3_embedding(x)\n          # x = layers.GRU(64, return_sequences=True) # stacking recurrent cells requires return_sequences=True\n          x = layers.GRU(64)(x)\n          # x = layers.Dense(64, activation="relu")(x) # optional dense layer after GRU cell\n          outputs = layers.Dense(1, activation="sigmoid")(x)\n          model_3 = tf.keras.Model(inputs, outputs, name="model_3_GRU")\n          ',language:"python",showLineNumbers:!1,theme:u.b}),l.a.createElement(d.a,null,"Bi-directional RNN"),l.a.createElement(u.a,{text:'\n          from tensorflow.keras import layers\n          model_4_embedding = layers.Embedding(input_dim=max_vocab_length,\n                                               output_dim=128,\n                                               embeddings_initializer="uniform",\n                                               input_length=max_length,\n                                               name="embedding_4")\n\n          # Build a Bidirectional RNN in TensorFlow\n          inputs = layers.Input(shape=(1,), dtype="string")\n          x = text_vectorizer(inputs)\n          x = model_4_embedding(x)\n          # x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x) # stacking RNN layers requires return_sequences=True\n          x = layers.Bidirectional(layers.LSTM(64))(x) # bidirectional goes both ways so has double the parameters of a regular LSTM layer\n          outputs = layers.Dense(1, activation="sigmoid")(x)\n          model_4 = tf.keras.Model(inputs, outputs, name="model_4_Bidirectional")\n          ',language:"python",showLineNumbers:!1,theme:u.b}),l.a.createElement(d.a,null,"Conv 1D"),l.a.createElement(u.a,{text:'\n          from tensorflow.keras import layers\n          model_5_embedding = layers.Embedding(input_dim=max_vocab_length,\n                                               output_dim=128,\n                                               embeddings_initializer="uniform",\n                                               input_length=max_length,\n                                               name="embedding_5")\n\n          # Create 1-dimensional convolutional layer to model sequences\n          from tensorflow.keras import layers\n          inputs = layers.Input(shape=(1,), dtype="string")\n          x = text_vectorizer(inputs)\n          x = model_5_embedding(x)\n          x = layers.Conv1D(filters=32, kernel_size=5, activation="relu")(x)\n          x = layers.GlobalMaxPool1D()(x)\n          # x = layers.Dense(64, activation="relu")(x) # optional dense layer\n          outputs = layers.Dense(1, activation="sigmoid")(x)\n          model_5 = tf.keras.Model(inputs, outputs, name="model_5_Conv1D")\n\n          # Compile Conv1D model\n          model_5.compile(loss="binary_crossentropy",\n                          optimizer=tf.keras.optimizers.Adam(),\n                          metrics=["accuracy"])\n\n          # Get a summary of our 1D convolution model\n          model_5.summary()\n          ',language:"python",showLineNumbers:!1,theme:u.b})))}},{path:"/article/rnn-2",component:function(e){return l.a.createElement("div",null,l.a.createElement(w,{title:"Recurrent Neural Network - Time series",backUrl:"/"}),l.a.createElement("div",{className:"article-content"},l.a.createElement(d.a,null,"Recurrent Neural Network - Time series"),l.a.createElement(d.a,{className:"mt-16"},"Windowing"),l.a.createElement(u.a,{text:'\n          HORIZON = 1\n          WINDOW_SIZE = 7\n\n          def get_labelled_windows(x, horizon=1):\n          """\n          Creates labels for windowed dataset.\n\n          E.g. if horizon=1 (default)\n          Input: [1, 2, 3, 4, 5, 6] -> Output: ([1, 2, 3, 4, 5], [6])\n          """\n          return x[:, :-horizon], x[:, -horizon:]\n\n          test_window, test_label = get_labelled_windows(tf.expand_dims(tf.range(8)+1, axis=0), horizon=HORIZON)\n          print(f"Window: {tf.squeeze(test_window).numpy()} -> Label: {tf.squeeze(test_label).numpy()}")\n\n\n          def make_windows(x, window_size=7, horizon=1):\n            window_step = np.expand_dims(np.arange(window_size+horizon), axis=0)\n            window_indexes = window_step + np.expand_dims(np.arange(len(x)-(window_size+horizon-1)), axis=0).T # create 2D array of windows of size window_size\n            windowed_array = x[window_indexes]\n            windows, labels = get_labelled_windows(windowed_array, horizon=horizon)\n            return windows, labels\n\n          full_windows, full_labels = make_windows(prices, window_size=WINDOW_SIZE, horizon=HORIZON)\n          len(full_windows), len(full_labels)\n\n          def make_train_test_splits(windows, labels, test_split=0.2):\n            split_size = int(len(windows) * (1-test_split))\n            train_windows = windows[:split_size]\n            train_labels = labels[:split_size]\n            test_windows = windows[split_size:]\n            test_labels = labels[split_size:]\n            return train_windows, test_windows, train_labels, test_labels\n\n          train_windows, test_windows, train_labels, test_labels = make_train_test_splits(full_windows, full_labels)\n          len(train_windows), len(test_windows), len(train_labels), len(test_labels)\n\n          import os\n\n          def create_model_checkpoint(model_name, save_path="model_experiments"):\n            return tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(save_path, model_name),\n                                                      verbose=0,\n                                                      save_best_only=True)\n          import tensorflow as tf\n          from tensorflow.keras import layers\n\n          model_1 = tf.keras.Sequential([\n            layers.Dense(128, activation="relu"),\n            layers.Dense(HORIZON, activation="linear")\n          ], name="model_1_dense")\n\n          model_1.compile(loss="mae",\n                          optimizer=tf.keras.optimizers.Adam(),\n                          metrics=["mae"])\n\n          model_1.fit(x=train_windows,\n                      y=train_labels,\n                      epochs=100,\n                      verbose=1,\n                      batch_size=128,\n                      validation_data=(test_windows, test_labels),\n                      callbacks=[create_model_checkpoint(model_name=model_1.name)])\n\n          model_1.evaluate(test_windows, test_labels)\n\n          def make_preds(model, input_data):\n            forecast = model.predict(input_data)\n            return tf.squeeze(forecast)\n\n          model_1_preds = make_preds(model_1, test_windows)\n          len(model_1_preds), model_1_preds[:10]\n\n\n          ',language:"python",showLineNumbers:!1,theme:u.b}),l.a.createElement(d.a,null,"Windowing + LSTM"),l.a.createElement(u.a,{text:'\n          inputs = layers.Input(shape=(WINDOW_SIZE))\n          x = layers.Lambda(lambda x: tf.expand_dims(x, axis=1))(inputs)\n          # x = layers.LSTM(128, activation="relu", return_sequences=True)(x) # this layer will error if the inputs are not the right shape\n          x = layers.LSTM(128, activation="relu")(x) # using the tanh loss function results in a massive error\n          # x = layers.Dense(32, activation="relu")(x)\n          output = layers.Dense(HORIZON)(x)\n          model_5 = tf.keras.Model(inputs=inputs, outputs=output, name="model_5_lstm")\n\n          model_5.compile(loss="mae",\n                          optimizer=tf.keras.optimizers.Adam())\n\n          model_5.fit(train_windows,\n                      train_labels,\n                      epochs=100,\n                      verbose=0,\n                      batch_size=128,\n                      validation_data=(test_windows, test_labels),\n                      callbacks=[create_model_checkpoint(model_name=model_5.name)])\n          ',language:"python",showLineNumbers:!1,theme:u.b}),l.a.createElement(d.a,null,"Time Series prediction in an open system is very hard & unpredictable as there are a lot of uncontrolled variables, whereas in a closed environment where everything is controlled, Windowing + RNNs give good results")))}}],v=a(58),_=a(97),x=a(18),k=a(170),N=a(189),z=a(186),S=a(98),C=a.n(S),T=(a(171),a(172),a(173),Object(k.a)((function(){return{width90:{width:"90%"},container:{width:"100%",marginBottom:"4px"}}})),a(188)),L=a(181),D=a(191),j=a(182),I=a(183),A=a(174),R=a(175),P=a(176),M=a(177),O=a(178),W=a(179),B=a(180),G=[{to:"skills",className:"navbar-skills",title:"Skills"},{to:"projects",className:"navbar-work",title:"Projects"},{to:"articles",className:"navbar-home",title:"Articles"},{to:"experience",className:"navbar-home",title:"Experience & Education"}],Z=Object(k.a)((function(){return{cursorPointer:{cursor:"pointer"},whiteBackground:{backgroundColor:"#ffffff"},navLinks:{width:"70%",alignItems:"center",justifyContent:"flex-end"},navLinksThin:{width:"40%",alignItems:"center",justifyContent:"flex-end"},navLinkText:{marginLeft:"20px",cursor:"pointer"}}})),V=function(){var e=Z(),t=Object(p.a)("(max-width:600px)"),a=Object(n.useState)(!1),r=Object(v.a)(a,2),i=r[0],o=r[1],s=function(){o(!i)};return l.a.createElement(l.a.Fragment,null,l.a.createElement(h.a,{position:"fixed",className:e.whiteBackground},l.a.createElement(g.a,null,l.a.createElement(N.a,{display:"flex",className:"width-100-percent"},l.a.createElement(N.a,{className:t?"width-60-percent":"width-30-percent"},l.a.createElement(x.Link,{activeClass:"active",className:"test1",to:"home",spy:!0,smooth:!0,duration:500},l.a.createElement(d.a,{className:e.cursorPointer,variant:"h5",color:"primary"},"PB Pranav Kumar"))),l.a.createElement(N.a,{display:"flex",className:t?e.navLinksThin:e.navLinks},t?l.a.createElement(B.a,{onClick:s,className:e.cursorPointer,color:"primary"}):l.a.createElement(l.a.Fragment,null,null===G||void 0===G?void 0:G.map((function(t){return l.a.createElement(x.Link,{activeClass:"active",className:"test1",to:t.to,spy:!0,smooth:!0,duration:500},l.a.createElement(d.a,{variant:"h5",color:"secondary",className:"".concat(t.className," ").concat(e.navLinkText)},t.title))}))))))),l.a.createElement(T.a,{anchor:"right",open:i,onClose:s},l.a.createElement(N.a,{className:"width-250-px"},l.a.createElement(L.a,null,G.map((function(e,t){var a=e.title,n=e.to;return l.a.createElement(x.Link,{activeClass:"active",className:"test1",to:n,onClick:s,spy:!0,smooth:!0,duration:500},l.a.createElement(D.a,{button:!0,key:a},l.a.createElement(j.a,null,function(e){switch(e){case 0:return l.a.createElement(A.a,null);case 1:return l.a.createElement(R.a,null);case 2:return l.a.createElement(P.a,null);case 3:return l.a.createElement(M.a,null);case 4:return l.a.createElement(O.a,null);case 5:return l.a.createElement(W.a,null);default:return l.a.createElement(A.a,null)}}(t)),l.a.createElement(I.a,{primary:a})))}))))))},F=a(106),q=function(e){var t=e.className,a=void 0===t?"":t,n=e.imgSrc,r=e.title,i=e.desc,o=e.link,s=e.linkTxt;return l.a.createElement(F.a,{className:a,style:{margin:"2%"},variant:"outlined",elevation:3},l.a.createElement(N.a,{style:{width:"350px"}},l.a.createElement("img",{style:{height:"250px"},src:"".concat(n),alt:"text",className:"paper_image"}),l.a.createElement(N.a,{m:1,display:"flex",style:{flexDirection:"column",justifyContent:"center"}},l.a.createElement(d.a,{variant:"h5",color:"primary"},"".concat(r)),l.a.createElement(d.a,{variant:"body1",style:{marginTop:"5px"}},"".concat(i)),l.a.createElement(d.a,{style:{marginTop:"5px"}},l.a.createElement("a",{href:"".concat(o),target:"_blank",rel:"noopener noreferrer",className:"text-blue"},"".concat(s))))," "))},U=a(184),X=function(e){var t=e.className,a=void 0===t?"":t,n=e.imgSrc,r=e.title,i=e.desc,o=e.link;return l.a.createElement(F.a,{className:a,style:{margin:"2%"},variant:"outlined",elevation:3},l.a.createElement(N.a,{style:{width:"375px"}},l.a.createElement("img",{style:{height:"250px",margin:"4px"},src:"".concat(n),alt:"text",className:"paper_image"}),l.a.createElement(N.a,{m:1,display:"flex",style:{height:"175px",flexDirection:"column",justifyContent:"space-between"}},l.a.createElement(d.a,{variant:"h5",color:"primary"},"".concat(r)),l.a.createElement(d.a,{variant:"body1",style:{marginTop:"5px"}},"".concat(i)),l.a.createElement(N.a,{display:"flex",style:{justifyContent:"center",marginTop:"5px",marginBottom:"5px"}},l.a.createElement(U.a,{variant:"contained",target:"_blank",href:o},"Open Article")))," "))},H=function(e){var t=e.isMaxWidth600,a=void 0!==t&&t,n=e.classes,r=void 0===n?{}:n,i=e.heroSrc,o=void 0===i?"":i;return l.a.createElement(N.a,{mt:10,p:a?2:0,display:"flex",alignItems:"center",justifyContent:"center"},l.a.createElement(f.a,{container:!0,component:N.a},l.a.createElement(f.a,{item:!0,md:7},l.a.createElement(N.a,{ml:a?0:4,display:"flex",flexDirection:"column",height:"100%",justifyContent:"center"},l.a.createElement(d.a,{variant:"h6",className:r.title},"I like developing software products"),l.a.createElement(d.a,{variant:"body1",className:r.titleDescription},"I like taking on challenging problems and solving them. In my 4 years of experience as a Software Developer, I\u2019ve contributed to the development of two SaaS products at an early-stage startup. For the past couple of years, I\u2019ve been building predictive models on kaggle. I started my Masters at UC in Artificial Intelligence in Aug\u201922 and I'll be graduating in Aug'23"),l.a.createElement(N.a,{display:"flex",className:r.flexFlowWrap},l.a.createElement(x.Link,{activeClass:"active",className:"test1",to:"projects",spy:!0,smooth:!0,duration:500},l.a.createElement(U.a,{variant:"contained",className:r.titleBtn},"Projects")),l.a.createElement(U.a,{variant:"contained",className:"".concat(r.ml8," ").concat(r.titleBtn),target:"_blank",href:"https://github.com/pbpranavk?tab=repositories"},"Github"),l.a.createElement(U.a,{variant:"contained",className:"".concat(r.ml8," ").concat(r.titleBtn),target:"_blank",href:"https://www.kaggle.com/pranavcoder"},"Kaggle"),l.a.createElement(U.a,{variant:"contained",className:"".concat(r.ml8," ").concat(r.titleBtn),target:"_blank",rel:"noopener noreferrer",href:"https://drive.google.com/file/d/11sHc-XAP1jhg4apyVZ6sR2_nwE4XpcH9/view?usp=sharing"},"My Resume")))),l.a.createElement(f.a,{item:!0,md:5},l.a.createElement(N.a,{display:"flex",mt:2,pr:a?0:4,justifyContent:a?"flex-start":"flex-end",alignItems:"center",height:"100%"},l.a.createElement("img",{style:{width:"90%"},src:o,alt:"home"})))))},J=a(185),K=a(190),Y=a(6),Q=Object(Y.a)((function(e){return{root:{height:23,borderRadius:5,width:"100%"},colorPrimary:{backgroundColor:"secondary"},bar:{borderRadius:5,backgroundColor:"#e38e54"}}}))(J.a),$=Object(k.a)({skillsContent:{justifyContent:"center",alignItems:"center"},skillBox:{width:"50%",justifyContent:"center"},skillBoxConfidence:{width:"30%",justifyContent:"center"}}),ee=function(e){var t=e.isLeft,a=void 0!==t&&t,n=e.skillName,r=e.confidenceLevel,i=$(),o=Object(p.a)("(max-width:600px)"),s=Object(p.a)("(max-width:1200px)");return o?l.a.createElement("div",{className:"flex-justify-content-center"},l.a.createElement(K.a,{className:"mx-8 mt-8",label:n})):l.a.createElement(N.a,{mt:4,display:"flex",className:"".concat(i.skillsContent," ").concat(s?"justify-content-center":a?"justify-content-end":"justify-content-start")},l.a.createElement(N.a,{display:"flex",className:"width-85-percent",alignItems:"center"},l.a.createElement(N.a,{display:"flex",style:{width:"70%"}},l.a.createElement(d.a,{variant:"h5",color:"secondary"},n)),l.a.createElement(N.a,{display:"flex",className:i.skillBoxConfidence},l.a.createElement(Q,{variant:"determinate",value:r}))))},te=function(e){var t=e.classes,a=void 0===t?{justifyContentCenter:""}:t,n=Object(p.a)("(max-width:600px)");return l.a.createElement(N.a,{mt:10},l.a.createElement(N.a,{display:"flex",className:a.justifyContentCenter},l.a.createElement(d.a,{variant:"h3",style:{fontWeight:"bold",fontSize:"40px"},color:"primary"},"My Skills")),l.a.createElement(f.a,{className:"mt-16",container:!0,spacing:n?0:4},l.a.createElement(f.a,{item:!0,xs:12,lg:6},l.a.createElement(ee,{skillName:"Python",confidenceLevel:85,isLeft:!0}),l.a.createElement(ee,{skillName:"React",confidenceLevel:85,isLeft:!0}),l.a.createElement(ee,{skillName:"Data Structures & Algorithms",confidenceLevel:75,isLeft:!0})),l.a.createElement(f.a,{item:!0,xs:12,lg:6},l.a.createElement(ee,{skillName:"EDA & Predictive Modelling",confidenceLevel:55}),l.a.createElement(ee,{skillName:"Tensorflow, Keras & Neural Nets",confidenceLevel:75}),l.a.createElement(ee,{skillName:"APIs, RDBMS, OOP-Design Pattens",confidenceLevel:55}))))},ae=function(e){var t=e.classes,a=void 0===t?{}:t;return l.a.createElement(l.a.Fragment,null,l.a.createElement(N.a,{mt:10},l.a.createElement(N.a,{display:"flex",className:a.justifyContentCenter},l.a.createElement(d.a,{variant:"h3",className:a.sectionTitle,color:"primary"},"Projects")),l.a.createElement(N.a,{display:"flex",className:a.sectionContent},l.a.createElement(q,{imgSrc:"fruit_veggies.jpeg",title:"Fruit or Vegetable",desc:"A Convoluted Neural Network (CNN) written in Tensorflow on a 2GB\n             dataset of fruits and vegetables. This is a multi-class classifier with\n              more than 35 classes.",link:"https://www.kaggle.com/pranavcoder/fruit-or-vegetable",linkTxt:"View on Kaggle"}),l.a.createElement(q,{imgSrc:"news.jpeg",title:"What\u2019s the NEWS",desc:"A Recurrent Neural Network (RNN) written in PyTorch\n             on a dataset of 200K news headlines from 2012 to 2018 from HuffPost.\n              This model can tag untagged news articles as well as it can identify\n               the type of language used in an article.",link:"https://www.kaggle.com/pranavcoder/what-s-the-news",linkTxt:"View on Kaggle"}),l.a.createElement(q,{imgSrc:"heart.jpeg",title:"Heart Attack Classification",desc:"Even though neural networks work very well, they need a lot of data. In this notebook I use\n             classic Machine Learning algorithms to predict if a person will get a heart attack or not.\n            ",link:"https://www.kaggle.com/pranavcoder/heart-attack-classification",linkTxt:"View on Kaggle"}),l.a.createElement(q,{imgSrc:"django-react-k8s.png",title:"A fully orchestrated app",desc:"A full-stack app built with Django, django-rest-framework, react.\n             I containerized it using docker and wrote yml scripts for deployments,\n              services to create resources in Kubernetes. I've written those scripts\n               for django main service(react is served from it), MySQL, Postgres & Redis.",link:"https://github.com/pbpranavk/full_stack_app_with_orchestration",linkTxt:"Check it out on Github"}),l.a.createElement(q,{imgSrc:"gradientCards2.png",title:"Gradient Cards",desc:"A set of cards with smooth gradients that go easy on the eye as well as\n             attract focus which is built with React. These can be used when you want\n              to style your UI with rather vibrant colours to provide a soothing\n               effect to the user.",link:"https://8xzcw.csb.app/",linkTxt:"View deployed version"}),l.a.createElement(q,{imgSrc:"spidywebar.png",title:"Augmented Reality on the web",desc:"A marker based Augmented Reality app for the web created with react and aframe.",link:"https://github.com/pbpranavk/archive/tree/master/spidyWebAR",linkTxt:"View on Github"}))))},ne=a(41),le=a(61),re=a.n(le),ie=a(74),oe=a.n(ie),se=a(100),ce=a.n(se),me=(a(143),function(e){var t=e.icon,a=void 0===t?l.a.createElement(l.a.Fragment,null):t,n=e.title,r=void 0===n?"":n,i=e.shortDesc,o=void 0===i?"":i,s=e.date,c=void 0===s?"":s,m=e.description,d=void 0===m?"":m,u=e.keyResponsibilities,p=void 0===u?[]:u;return l.a.createElement(ne.VerticalTimelineElement,{className:"vertical-timeline-element--work",contentStyle:{background:"rgb(33, 150, 243)",color:"#fff"},contentArrowStyle:{borderRight:"7px solid  rgb(33, 150, 243)"},date:c,iconStyle:{background:"rgb(33, 150, 243)",color:"#fff"},icon:a},l.a.createElement("h3",{className:"vertical-timeline-element-title"},r),l.a.createElement("h4",{className:"vertical-timeline-element-subtitle"},"".concat(o," ").concat(c)),l.a.createElement("p",null,d," "),l.a.createElement("p",null,"Key responsibilities include:"),l.a.createElement("ul",{className:"experience-ul",style:{marginTop:"0px"}},null===p||void 0===p?void 0:p.map((function(e){return l.a.createElement("li",null,e)}))))}),de=function(e){var t=e.classes,a=void 0===t?{}:t;return l.a.createElement(N.a,{mt:10},l.a.createElement(N.a,{display:"flex",className:a.justifyContentCenter},l.a.createElement(d.a,{variant:"h3",className:a.sectionTitle,color:"primary"},"Experience & Education")),l.a.createElement(ne.VerticalTimeline,{lineColor:"#222"},l.a.createElement(ne.VerticalTimelineElement,{className:"vertical-timeline-element--education",date:"Aug 2022 - Aug 2023",iconStyle:{background:"rgb(233, 30, 99)",color:"#fff"},icon:l.a.createElement(oe.a,null)},l.a.createElement("h3",{className:"vertical-timeline-element-title"},"Masters in Artificial Intelligence from University of Cincinnati"),l.a.createElement("h4",{className:"vertical-timeline-element-subtitle"},"CGPA: 4.0 / 4.0"),l.a.createElement("p",null,"Subjects Taken: Deep Learning, Intelligent Systems, Complex Systems & Networks, Intro to Applied AI and ML Tools, Venture Capital, Data Encoding, Operating Systems")),l.a.createElement(me,{icon:l.a.createElement(re.a,null),title:"Senior Software Engineer",shortDesc:"Beautiful Code LLP",date:"(May 2020 \u2013 Present)",description:"I've contributed to the architecture, development and maintenance of  SaaS products at easy.xyz (Easy is a sister brand of BeautifulCode)",keyResponsibilities:["Ownership of Frontend codebase","Scaling the apps to 2000 active users and ensuring a smooth experience by incorporating Design Patterns and Scalability (both vertical & horizontal)","Guiding Junior Developers through code reviews","Interacting with Product Owners to develop bug-free robust code"]}),l.a.createElement(me,{icon:l.a.createElement(re.a,null),title:"Software Engineer",shortDesc:"Beautiful Code LLP",date:"(June 2019 \u2013 May 2020)",description:"Worked for a US-based client to build a SaaS product involving Django, React & K8s",keyResponsibilities:["Developing robust APIs and Database Models","Designing asynchronous microservices using python","Building React components for dashboards with graphs and tables","Interacting with clients"]}),l.a.createElement(me,{icon:l.a.createElement(re.a,null),title:"Associate Engineer",shortDesc:"Kony IT Services Pvt Ltd",date:"(June 2018 \u2013 June 2019)",description:"As an Associate Engineer, I worked on building a cross-platform mobile app (Android & iOS) for an Indian Bank. This was during my final year at college, I had to balance my work and study. I was one of the 6 students selected from our batch",keyResponsibilities:["Optimizing existing APIs","Developing mobile app screens","Troubleshooting & fixing issues from legacy code."]}),l.a.createElement(ne.VerticalTimelineElement,{className:"vertical-timeline-element--education",date:"2015 - 2019",iconStyle:{background:"rgb(233, 30, 99)",color:"#fff"},icon:l.a.createElement(oe.a,null)},l.a.createElement("h3",{className:"vertical-timeline-element-title"},"Bachelor of Technology in Computer science and Engineering"),l.a.createElement("h4",{className:"vertical-timeline-element-subtitle"},"CGPA: 3.3 / 4.0"),l.a.createElement("p",null,"Subjects Taken: Python Programming, Data Structures and Algorithms, Design Patterns, Operating Systems, Computer Networks, Big Data Analytics, Data Visualization, Machine Learning & Deep Learning")),l.a.createElement(ne.VerticalTimelineElement,{iconStyle:{background:"rgb(16, 204, 82)",color:"#fff"},icon:l.a.createElement(ce.a,null)})))},ue=a(101),pe=a.n(ue),he=a(102),ge=a.n(he),fe=a(103),ye=a.n(fe),Ee=function(e){return l.a.createElement("div",{className:"flex footer mt-24"},l.a.createElement(U.a,{href:"https://github.com/pbpranavk/",target:"_blank",rel:"noopener noreferrer"},l.a.createElement(pe.a,{className:"footer-icon margin-right-10px wheat-color"})),l.a.createElement(U.a,{href:"https://twitter.com/pbpranav24/",target:"_blank",rel:"noopener noreferrer"},l.a.createElement(ge.a,{className:"footer-icon margin-right-10px wheat-color"})),l.a.createElement(U.a,{href:"https://www.linkedin.com/in/p-b-pranav-kumar/",target:"_blank",rel:"noopener noreferrer"},l.a.createElement(ye.a,{className:"footer-icon wheat-color"})))},we=[{key:1,imgSrc:"snn.svg",title:"Foundations of Neural Nets",desc:"This article covers how to build a very basic Neural Network\n            with layers and activation function.\n            ",link:"https://www.linkedin.com/pulse/foundations-neural-nets-pranav-kumar-pb/"},{key:2,imgSrc:"dnn.svg",title:"Deep Neural Nets & Improving them",desc:"Extending Shallow neural nets to multiple hidden layers\n  and improving their accuracy",link:"https://www.linkedin.com/pulse/deep-neural-nets-improving-them-pranav-kumar-pb/"},{key:3,imgSrc:"cnn.svg",title:"Convolutions-Pooling-Flattening",desc:"Using Convolutions and pooling to build Neural Networks that can\n            deal better with image data.",link:"https://www.linkedin.com/pulse/convolutions-pooling-flattening-pranav-kumar-pb/"},{key:4,imgSrc:"rnn-1.svg",title:"Backprop through time",desc:"Using Memory (LSTM & GRUs) Networks and Word2Vec to work on NLP Tasks",link:"https://www.linkedin.com/pulse/backprop-through-time-pranav-kumar-pb/"}],be=function(e){var t=e.classes,a=void 0===t?{}:t;return l.a.createElement(l.a.Fragment,null,l.a.createElement(N.a,{mt:10},l.a.createElement(N.a,{display:"flex",className:a.justifyContentCenter},l.a.createElement(d.a,{variant:"h3",className:a.sectionTitle,color:"primary"},"Articles")),l.a.createElement(N.a,{display:"flex",className:"".concat(a.sectionContent," article-container"),style:{overflowX:"auto",overflowY:"hidden",height:"520px",flexWrap:"nowrap",justifyContent:"flex-start",width:"95%",margin:"auto"}},null===we||void 0===we?void 0:we.map((function(e){return l.a.createElement(X,{id:null===e||void 0===e?void 0:e.key,imgSrc:null===e||void 0===e?void 0:e.imgSrc,title:null===e||void 0===e?void 0:e.title,desc:null===e||void 0===e?void 0:e.desc,link:null===e||void 0===e?void 0:e.link})})))))},ve=a(104),_e=a.n(ve),xe=a(69);function ke(){var e=Object(_.a)(["\n  display: block;\n  margin: 50px 20%;\n  border-color: red;\n"]);return ke=function(){return e},e}var Ne=Object(xe.css)(ke()),ze=Object(k.a)((function(e){return{mt:{marginTop:"8px"},mb:{marginBottom:"8px"},ml8:{marginLeft:"8px"},mx10:{marginRight:"5%",marginLeft:"5%"},primaryDivider:{backgroundColor:"#E86410"},textAlign:{textAlign:"justify"},workCardPadding:{},title:{color:"#000",fontSize:"35px"},titleDescription:{color:"#A4A4A4",fontSize:"28px",marginTop:"10px"},flexFlowWrap:{flexFlow:"wrap"},titleBtn:{marginTop:"40px",backgroundColor:"rgba(232, 100, 16, 0.69)"},justifyContentCenter:{justifyContent:"center"},sectionTitle:{fontWeight:"bold",fontSize:"40px"},sectionContent:{flexWrap:"wrap",justifyContent:"center"}}})),Se=function(e){var t=ze(),a=Object(n.useState)(!0),r=Object(v.a)(a,2),i=r[0],o=r[1],s=Object(p.a)("(max-width:600px)");return Object(n.useEffect)((function(){setTimeout((function(){o(!1);var e=document.getElementById("after-loader");e&&(e.className="after-loaded-visible");var t=document.getElementById("loader");t&&(t.className=" display-none")}),1e3)}),[]),i?l.a.createElement("div",{className:"loader",id:"loader"},l.a.createElement(C.a,{css:Ne,size:150,margin:1,color:"#000000",loading:i})):l.a.createElement("div",{className:"App"},l.a.createElement("div",{style:{maxWidth:"1440px",margin:"auto"}},l.a.createElement(V,null),l.a.createElement(N.a,null,l.a.createElement(x.Element,{name:"home"},l.a.createElement(H,{isMaxWidth600:s,classes:t,heroSrc:_e.a})),l.a.createElement(x.Element,{name:"skills"},l.a.createElement(te,{classes:t})),l.a.createElement(x.Element,{name:"projects"},l.a.createElement(ae,{classes:t})),l.a.createElement(x.Element,{name:"articles"},l.a.createElement(be,{classes:t})),l.a.createElement(x.Element,{name:"experience"},l.a.createElement(de,{classes:t})))),l.a.createElement(z.a,null),l.a.createElement(Ee,null))},Ce=Object(o.a)({palette:{primary:{main:"#E86410"},secondary:{main:"#DDAB8B"}},typography:{fontFamily:'"Comic Sans MS"'}});var Te=function(){return l.a.createElement(s.a,{theme:Ce},l.a.createElement(c.a,null,l.a.createElement(m.c,null,l.a.createElement(m.a,{exact:!0,path:"/"},l.a.createElement(Se,null)),null===b||void 0===b?void 0:b.map((function(e){return l.a.createElement(m.a,{path:e.path,exact:!0},e.component)})),l.a.createElement(m.a,{path:"*",exact:!0},"Not found"))))};Boolean("localhost"===window.location.hostname||"[::1]"===window.location.hostname||window.location.hostname.match(/^127(?:\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/));i.a.render(l.a.createElement(l.a.StrictMode,null,l.a.createElement(Te,null)),document.getElementById("root")),"serviceWorker"in navigator&&navigator.serviceWorker.ready.then((function(e){e.unregister()})).catch((function(e){console.error(e.message)}))},36:function(e,t,a){}},[[113,152,153]]]);
//# sourceMappingURL=main.7412af93.chunk.js.map